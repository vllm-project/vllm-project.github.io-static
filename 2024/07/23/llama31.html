<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Announcing Llama 3.1 Support in vLLM | vLLM Blog</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Announcing Llama 3.1 Support in vLLM" />
<meta name="author" content="vLLM Team" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Today, the vLLM team is excited to partner with Meta to announce the support for the Llama 3.1 model series. Llama 3.1 comes with exciting new features with longer context length (up to 128K tokens), larger model size (up to 405B parameters), and more advanced model capabilities. The vLLM community has added many enhancements to make sure the longer, larger Llamas run smoothly on vLLM, which includes chunked prefill, FP8 quantization, and pipeline parallelism. We will introduce these new enhancements in this blogpost." />
<meta property="og:description" content="Today, the vLLM team is excited to partner with Meta to announce the support for the Llama 3.1 model series. Llama 3.1 comes with exciting new features with longer context length (up to 128K tokens), larger model size (up to 405B parameters), and more advanced model capabilities. The vLLM community has added many enhancements to make sure the longer, larger Llamas run smoothly on vLLM, which includes chunked prefill, FP8 quantization, and pipeline parallelism. We will introduce these new enhancements in this blogpost." />
<link rel="canonical" href="http://localhost:4000/2024/07/23/llama31.html" />
<meta property="og:url" content="http://localhost:4000/2024/07/23/llama31.html" />
<meta property="og:site_name" content="vLLM Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-23T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Announcing Llama 3.1 Support in vLLM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"vLLM Team"},"dateModified":"2024-07-23T00:00:00-07:00","datePublished":"2024-07-23T00:00:00-07:00","description":"Today, the vLLM team is excited to partner with Meta to announce the support for the Llama 3.1 model series. Llama 3.1 comes with exciting new features with longer context length (up to 128K tokens), larger model size (up to 405B parameters), and more advanced model capabilities. The vLLM community has added many enhancements to make sure the longer, larger Llamas run smoothly on vLLM, which includes chunked prefill, FP8 quantization, and pipeline parallelism. We will introduce these new enhancements in this blogpost.","headline":"Announcing Llama 3.1 Support in vLLM","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/07/23/llama31.html"},"url":"http://localhost:4000/2024/07/23/llama31.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="vLLM Blog" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">vLLM Blog</a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Announcing Llama 3.1 Support in vLLM</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-07-23T00:00:00-07:00" itemprop="datePublished">
        Jul 23, 2024
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">vLLM Team</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Today, the vLLM team is excited to partner with Meta to announce the support for the Llama 3.1 model series. Llama 3.1 comes with exciting new features with longer context length (up to 128K tokens), larger model size (up to 405B parameters), and more advanced model capabilities. The vLLM community has added many enhancements to make sure the longer, larger Llamas run smoothly on vLLM, which includes chunked prefill, FP8 quantization, and pipeline parallelism. We will introduce these new enhancements in this blogpost.</p>

<h3 id="introduction">Introduction</h3>

<p>vLLM is a fast, easy-to-use, open-source serving engine for large language models. vLLM has support for more than 40 types of open-source LLMs, a diverse set of hardware platforms (Nvidia GPU, AMD GPU, AWS Inferentia, Google TPU, Intel CPU, GPU, Gaudi, …) and all kinds of inference optimizations. Learn more about vLLM <a href="https://docs.vllm.ai/">here</a>.</p>

<p>For the new Llama 3.1 series, vLLM can run the models with a full 128K context window. In order to support a large context window, vLLM automatically enables <a href="https://www.linkedin.com/posts/joinanyscale_recently-weve-contributed-chunked-prefill-activity-7201277641490849792-lGqZ">chunked prefill</a>. Chunked prefill not only keeps memory usage under control, but also reduces the interruption from long prompt processing for ongoing requests. You can install vLLM by running the following command or using our official docker image (<code class="language-plaintext highlighter-rouge">vllm/vllm-openai</code>):</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-U</span> vllm
</code></pre></div></div>

<p>For the large Llama 405B model, vLLM supports it in several methods:</p>
<ul>
  <li><strong>FP8:</strong> vLLM runs the official FP8 quantized model natively on 8xA100 or 8xH100.</li>
  <li><strong>Pipeline Parallelism:</strong> vLLM runs the official BF16 version on multiple nodes by placing different layers of the model on different nodes.</li>
  <li><strong>Tensor Parallelism:</strong> vLLM can also run by sharding the model across multiple nodes, and multiple GPUs within the nodes.</li>
  <li><strong>AMD MI300x or NVIDIA H200:</strong> vLLM can run the model on a single 8xMI300x or 8xH200 machine, where each GPU has 192GB and 141 GB memory, respectively.</li>
  <li><strong>CPU Offloading:</strong> as the last resort, vLLM can offload some of the weights to CPU while performing the forward pass, allowing you to run the large model at full precision on limited GPU memory.</li>
</ul>

<p>Please note that while vLLM supports all these methods, the performance is still preliminary. The vLLM community is actively working on optimizations and we welcome everyone’s contribution. For example, we are actively exploring more approaches to quantize the model, and to increase the throughput of pipeline parallelism. The performance numbers posted later in the blog are meant as early reference points; we expect the performance to improve significantly over the next few weeks.</p>

<p>Out of all the methods, we recommend FP8 for a single node, and pipeline parallelism for multiple nodes. Let’s discuss them in more detail.</p>

<h3 id="fp8">FP8</h3>

<p>FP8 represents float point numbers in 8 bits. The current generation of GPUs (H100, MI300x) provide native support for FP8 via specialized tensor cores. Currently, vLLM can run FP8 quantized models for KV cache, attention, and MLP layers. This reduces memory footprint, increases throughput, lowers latency, and comes with minimal accuracy drops.</p>

<p>Currently, vLLM supports the official Meta Llama 3.1 405B FP8 model quantized via FBGEMM by leveraging per-channel quantization in the MLP layer. In particular, each channel of the up/gate/down projections are quantized and multiplied by a static scaling factor. Combined with skipping quantization for the first and the last layer, and a static upper bound, this approach has minimal impact on the model’s accuracy. You can run the model with latest vLLM on a single 8xH100 or 8xA100 with the following command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vllm serve meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 <span class="nt">--tensor-parallel-size</span> 8
</code></pre></div></div>

<p>Using the FP8 quantized model serving requests with the average input length of 1024 tokens and the average output length of 128 output tokens, the server can sustain 2.82 requests per second. The corresponding serving throughput is 2884.86 input tokens per second and 291.53 output tokens per seconds, respectively.</p>

<p>We also independently confirmed the accuracy drop of the FP8 checkpoints is minimal. For example, running the GSM8K benchmark using lm-eval-harness with 8 shots and chain-of-thought, we observed the exact match score of 95.38% (+- 0.56% stddev), which is a minimal drop compared to the BF16 official score of 96.8%.</p>

<h3 id="pipeline-parallelism">Pipeline Parallelism</h3>

<p>What if you want to run the Llama 3.1 405B model without quantization? You can do it with 16xH100 or 16xA100 GPUs using vLLM’s pipeline parallelism!</p>

<p>Pipeline parallelism splits a model into smaller sets of layers, executing them in parallel on two or more nodes in a pipelined fashion. Unlike tensor parallelism, which requires expensive all-reduce operations, pipeline parallelism partitions the model across layer boundaries, needing only inexpensive point-to-point communication. This is particularly useful when you have multiple nodes that are not necessarily connected via fast interconnects like Infiniband.</p>

<p>vLLM supports combining pipeline and tensor parallelism. For example, with 16 GPUs across 2 nodes, you can use 2-way pipeline parallelism and 8-way tensor parallelism to optimize hardware usage. This configuration maps half the model to each node, partitioning each layer across 8 GPUs using NVLink for all-reduce operations. You can run the Llama 3.1 405B model with the following command:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vllm serve meta-llama/Meta-Llama-3.1-405B-Instruct <span class="nt">--tensor-parallel-size</span> 8 <span class="nt">--pipeline-parallel-size</span> 2
</code></pre></div></div>

<p>If you have fast interconnects like Infiniband, you can use 16-way tensor parallelism:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>vllm serve meta-llama/Meta-Llama-3.1-405B-Instruct <span class="nt">--tensor-parallel-size</span> 16
</code></pre></div></div>

<p align="center">
<picture>
<img src="/assets/figures/llama31/perf_llama3.png" width="50%" />
</picture>
<br />Serving throughput on 16xH100 GPUs with a synthetic dataset (avg. input len 1024, avg. output len 128).
</p>

<p>We have observed that pipeline parallelism is essential when the nodes are not connected via Infiniband. Compared to 16-way tensor parallelism, combining 2-way pipeline parallelism with 8-way tensor parallelism leads to 6.6x performance improvements. On the other hand, with Infiniband, the performance of both configurations is similar.</p>

<p>To learn more about distributed inference using vLLM please refer to <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">this doc</a>. For CPU offloading, please refer to <a href="https://docs.vllm.ai/en/latest/getting_started/examples/cpu_offload.html">this example</a>.</p>

<p><br /></p>

<hr />

<h3 id="acknowledgements">Acknowledgements</h3>

<p>We would like to thank Meta for the pre-release partnership and letting us test the model. Independently from the release, we thank the following vLLM contributors for the features mentioned in this blogpost: <a href="https://neuralmagic.com/">Neural Magic</a> for FP8 quantization; <a href="https://centml.ai/">CentML</a> for pipeline parallelism; <a href="https://www.anyscale.com/">Anyscale</a> for the chunked prefill feature. The evaluation runs on <a href="https://lambdalabs.com/service/gpu-cloud/1-click-clusters">Lambda’s 1-Click Clusters</a> with InfiniBand, and we thank Lambda for the resource and the smooth cluster setup experience.</p>

  </div><a class="u-url" href="/2024/07/23/llama31.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p> -->
        <ul class="contact-list">
          <li class="p-name">© 2024. vLLM Team. All rights reserved.</li>
          <li><a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
