<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Structured Decoding in vLLM: a gentle introduction | vLLM Blog</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Structured Decoding in vLLM: a gentle introduction" />
<meta name="author" content="Guest Post by BentoML and Red Hat" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TL/DR:" />
<meta property="og:description" content="TL/DR:" />
<link rel="canonical" href="https://blog.vllm.ai/2025/01/14/struct-decode-intro.html" />
<meta property="og:url" content="https://blog.vllm.ai/2025/01/14/struct-decode-intro.html" />
<meta property="og:site_name" content="vLLM Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-14T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Structured Decoding in vLLM: a gentle introduction" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Guest Post by BentoML and Red Hat"},"dateModified":"2025-01-14T00:00:00-08:00","datePublished":"2025-01-14T00:00:00-08:00","description":"TL/DR:","headline":"Structured Decoding in vLLM: a gentle introduction","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.vllm.ai/2025/01/14/struct-decode-intro.html"},"url":"https://blog.vllm.ai/2025/01/14/struct-decode-intro.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.vllm.ai/feed.xml" title="vLLM Blog" /><script async src="https://www.googletagmanager.com/gtag/js?id=G-9C5R3JR3QS"></script>
<script>
  window['ga-disable-G-9C5R3JR3QS'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9C5R3JR3QS');
</script>

</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">vLLM Blog</a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Structured Decoding in vLLM: a gentle introduction</h1>
    <p class="post-meta"><time class="dt-published" datetime="2025-01-14T00:00:00-08:00" itemprop="datePublished">
        Jan 14, 2025
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Guest Post by BentoML and Red Hat</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><strong>TL/DR</strong>:</p>

<ul>
  <li>Structured decoding allows precise control over LLM output formats</li>
  <li>vLLM now supports both <a href="https://github.com/dottxt-ai/outlines">outlines</a> and <a href="https://github.com/mlc-ai/xgrammar">XGrammar</a> backends for structured decoding</li>
  <li>Recent XGrammar integration brings up to 5x improvement in time per output token (TPOT) under load</li>
  <li>Upcoming v1 release focuses on enhanced performance and schedule-level mask broadcasting for mixed-requests batch support</li>
</ul>

<p><em><a href="https://blog.vllm.ai/2023/06/20/vllm.html">vLLM</a> is the high-throughput and efficient inference engine for running <strong>large-language models</strong> (LLMs). In this post, we will explore the annotated history of language models, describe the current state of structured decoding in vLLM, as well as the recent integration with <a href="https://github.com/vllm-project/vllm/pull/10785">XGrammar</a>, and <a href="https://github.com/vllm-project/vllm/issues/8779">share our tentative roadmap for future improvements</a>.</em></p>

<blockquote>
  <p>We would also invite users to tackle this blog post from a philosophical perspective, and in the process trying to posit that structured decoding represents a fundamental shift in how we think about LLM outputs. It also plays an important role in building complex agentic system.</p>
</blockquote>

<p>For more information about vLLM, please check out our <a href="https://docs.vllm.ai/en/latest/">documentation</a>.</p>

<h2 id="language-models-a-brief-historical-context">Language models: A brief historical context</h2>

<p>In 1950, Alan Turing proposed that a high-speed digital computer, programmed with rules, could exhibit emergent behaviour of intelligence (Turing, 1950). This led to two main approaches in AI development:</p>

<ol>
  <li>
    <p>Good Old-Fashioned AI (GOFAI): A paradigm quickly emerged among researchers in the 1950s, where expert systems were designed to replicate the decision-making capabilities of a human specialist<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, (or symbolic reasoning system), referred to by Haugland as Good Old-Fashioned AI (GOFAI) (Haugeland, 1997). However, it quickly ran into funding problems due to its semantic representation not being able to scale up to generalised tasks (Also known as the “AI Winter” (Hendler, 2008)).</p>
  </li>
  <li>
    <p>New-Fangled AI (NFAI): Concurrently, Donald Norman’s Parallel Distributed Processing (Rumelhart et al., 1986) group investigated variations of Rosenblatt’s perception (Rosenblatt, 1958), where they proposed <em>hidden layers</em> within the network alongside with inputs and outputs to extrapolate appropriate responses based on what it had learned during training process. These connectionist networks were often built on top of statistical methods<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Given the abundance of data and Moore’s Law<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> resulting in an unprecedented amount of compute available, we see the complete dominance of connectionist networks in both research and production use-cases, most notably variants of <em>decoder-only</em> transformers<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> for <em>text generations</em> tasks. As such, most modern transformers variants are considered <strong>NFAI</strong> systems.</p>
  </li>
</ol>

<p>In summary:</p>

<ul>
  <li>GOFAI are <em>deterministic</em> and rule-based, given its intentionality is injected through explicit programming</li>
  <li>NFAI are often considered as “black-box” models (in: input - out: some output), data-driven given the networked complexity nature of its internal representations</li>
</ul>

<h2 id="why-do-we-need-structured-decoding">Why do we need structured decoding?</h2>

<figure>
  <img src="/assets/figures/struct-decode-intro/shogoth-gpt.png" />
<figcaption>
Shogoth as GPTs. In a sense, RLHF, or any post-training methods, is an injection of rules (a GOFAI system) into any large compound AI systems
</figcaption>
</figure>

<p>LLMs excel at the following heuristic: given a blob of text, the model will generate a contiguous piece of text that it predicts as the most probable tokens. For example, if you give it a Wikipedia article, the model should produce text consistent with the remainder of said article.</p>

<p>These models work well given the following assumption: the input prompt must be coherent and well-structured surrounding a given problem the users want to achieve. In other words, LLMs can be unpredictable when you need output in specific formats. Think of asking a model to generate JSON - without guidance, it might produce valid text that breaks JSON specification<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<p>This is where structured decoding comes in. It enables LLMs to generate outputs that follow a desired structure while preserving the non-deterministic nature of the system.</p>

<p>Companies like OpenAI have recognized this need, implementing features like <a href="https://platform.openai.com/docs/guides/structured-outputs#json-mode">JSON mode</a> to constrain<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> the output format. If you have built with these functionalities before (such as agentic workflows, function calling, coding assistant), chances are you are using structured decoding under the hood.</p>

<blockquote>
  <p>Guided decoding is to LLMs what <strong>validation</strong> is to APIs - it acts as a guarantee that what comes out matches what you expect. Guided decoding ensures structure integrity that allows developers to integrate LLMs into their application with ease!</p>
</blockquote>

<h2 id="structured-decoding-and-vllm">Structured decoding and vLLM</h2>

<p>In simple terms, structured decoding gives LLMs a “template” to follow. Users provide a schema that “influences” the model’s output, ensuring compliance with the desired structure:</p>

<p><img src="/assets/figures/struct-decode-intro/mermaid-intro.svg" alt="top level view of structure decoding" /></p>

<p>From a technical perspective, an inference engine can modify the probability distribution for next-tokens by applying bias (often via logit masks) for all tokens from any given schemas. To apply these biases, <a href="https://github.com/dottxt-ai/outlines">outlines</a> proposed guided generations via finite-state machine (FSM) for any given schemas (Willard &amp; Louf, 2023). This allows us to track the current state during decoding and filter out invalid tokens by applying logit bias to the output.</p>

<figure>
  <img src="/assets/figures/struct-decode-intro/constrained-json-fsm.webp" />
<figcaption>
courtesy of <a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/" target="_blank">LMSys, 2024</a>.
</figcaption>
</figure>

<p><em>in vLLM, you can use this by passing a JSON schema to the sampling params (either through Python SDK or HTTP requests).</em></p>

<blockquote>
  <p>Note: in some cases, it can even <a href="https://blog.dottxt.co/coalescence.html">improve</a> the native decoding performance for LLM!</p>
</blockquote>

<h3 id="previous-limitations-in-vllm">Previous limitations in vLLM</h3>

<p>There are few limitations with current vLLM’s support of the Outlines backend:</p>

<ol>
  <li><strong>Slow decoding</strong>: FSM has to be constructed at a token-level, meaning it can only transition the state one token per step. Therefore, it can only decode <em>one</em> token at a time, resulting in slow decoding.</li>
  <li><strong>Batch processing bottlenecks</strong>: Implementation in <a href="https://github.com/vllm-project/vllm/blob/80c751e7f68ade3d4c6391a0f3fce9ce970ddad0/vllm/model_executor/guided_decoding/outlines_logits_processors.py">vLLM</a> relies heavily on logit processor<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. As such, this is on the critical path of the sampling process. In batching use-case, compiling FSM per requests as well as computing the mask synchronous means that <strong>all requests</strong> in any given batches will get blocked, resulting in high time-to-first-tokens (TTFT) and lower throughput.
    <ul>
      <li>We found that compiling FSM is proven to be a relatively expensive task, making it a significant contributor to the increased TTFT.</li>
    </ul>
  </li>
  <li><strong>Performance issues with CFG mode</strong>: With outlines integrations, while JSON mode is relatively fast, the CFG mode runs significantly slower, and can occasionally <a href="https://github.com/vllm-project/vllm/issues/10081">crashes</a> the engine.</li>
  <li><strong>Limited advanced feature support</strong>: Techniques like <a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/">jump-forward decoding</a> are currently not possible with logit-processor approach. It requires prefilling a set of k-next tokens, whereas for logit processors we can only deal with the next-token.</li>
</ol>

<h3 id="integration-with-xgrammar">Integration with XGrammar</h3>

<p><a href="https://github.com/mlc-ai/xgrammar">XGrammar</a> introduces a new technique that batch constrained decoding via pushdown automaton (PDA). You can think of a PDA as a “collection of FSMs, and each FSM represents a context-free grammar (CFG).” One significant advantage of PDA is its recursive nature, allowing us to execute multiple state transitions. They also include additional <a href="https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar">optimisation</a> (for those who are interested) to reduce grammar compilation overhead.</p>

<p>This advancement addresses <strong>limitation (1)</strong> by moving grammar compilation out of Python into C, utilising <code class="language-plaintext highlighter-rouge">pthread</code>. Additionally, XGrammar lays the groundwork for addressing <strong>limitation (4)</strong> in future releases. Below are performance comparisons between the XGrammar and Outlines backends:</p>

<figure>
  <img src="/assets/figures/struct-decode-intro/vllm-new-xgrammar.png" />
  <img src="/assets/figures/struct-decode-intro/vllm-xgrammar-decode-time-per-output-token.png" />
<figcaption>
courtesy of Michael Goin (Red Hat).
</figcaption>
</figure>

<p>In vLLM’s v0 architecture, we’ve implemented XGrammar as a <a href="https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/guided_decoding/xgrammar_decoding.py">logit processor</a>, optimizing it with caching for tokenizer data. While the performance improvements are encouraging, we believe there’s still significant room for optimization.</p>

<p>There are still a few usability concerns in XGrammar v0 integration to match feature parity with all use cases:</p>

<ul>
  <li>It is yet to support grammars other than GBNF format (PR on vLLM: <a href="https://github.com/vllm-project/vllm/pull/10870">github</a>)</li>
  <li>It is yet to support regex</li>
  <li>It is yet to support complex JSON that uses regex patterns or numeric ranges
    <ul>
      <li>There are a few PR trying to cover this usage. There was one <a href="https://github.com/vllm-project/vllm/pull/10899">bugfix PR on vLLM</a> and one <a href="https://github.com/mlc-ai/xgrammar/pull/106">upstream</a></li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>vLLM now has a basic support for XGrammar by default. In case where we know XGrammar is insufficient to serve the request, we fall back to Outlines.</p>

  <p>Note that vLLM also includes support for lm-format-enforcer. However, from our testing we found that in some long context test cases, lm-format-enforcer fails to enforce correct outputs, and not up to par with Outlines in terms of performance.</p>
</blockquote>

<h2 id="tentative-plans-for-v1">Tentative plans for v1</h2>

<p>With the release of <a href="https://github.com/vllm-project/vllm/issues/8779">v1</a> on the horizon, we’re working on a tentative plan for structured decoding:</p>

<ol>
  <li>Moving guided decoding towards scheduler-level <a href="https://www.notion.so/Blog-4X-structured-decoding-speed-in-vLLM-8c3f2d44f6504202abbdb534983f2b2e?pvs=21">[10]</a>
    <ul>
      <li>Reason: We have more context regarding which requests that use structured decoding at a scheduler-level, therefore it shouldn’t block other requests within the batch (tentatively addressing <strong>limitation (2)</strong>). In a sense, this moves guided decoding outside of the critical path.</li>
      <li>This would allow for more natural vertical integration with jump-forward decoding (address <strong>limitation (4)</strong>).</li>
    </ul>
  </li>
  <li>Allowing bit-mask calculation in one process instead of each GPU workers
    <ul>
      <li>Reason: We can broadcast this bit-mask to each GPU worker instead of repeating this process per GPU worker.</li>
      <li>We will look to carefully analyze the bandwidth implications of broadcasting masks for every sample per request that use guided decoding.</li>
    </ul>
  </li>
  <li>Good baseline for speculative decoding and tool-use
    <ul>
      <li>Reason: XGrammar includes plans to support tool-use, such that we can move away from Python’s <a href="https://github.com/vllm-project/vllm/tree/main/vllm/entrypoints/openai/tool_parsers">tool parser</a>.</li>
      <li>Tree scoring in speculative decoding can then use the same API as jump-forward decoding (which depends on the integration of guided decoding at the scheduler level).</li>
    </ul>
  </li>
</ol>

<p><em>NOTE: if you have any more suggestions we are more than happy to take it into consideration. Consider joining <a href="https://www.notion.so/bentoml/slack.vllm.ai">vLLM slack</a> via <code class="language-plaintext highlighter-rouge">#feat-structured-output</code>.</em></p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We want to thank the vLLM team, XGrammar team, <a href="https://github.com/aarnphm">Aaron Pham (BentoML)</a>, <a href="https://github.com/mgoin">Michael Goin (Red Hat)</a>, <a href="https://github.com/xuechendi">Chendi Xue (Intel)</a>, and <a href="https://github.com/russellb">Russell Bryant (Red Hat)</a> for their valuable feedback and collaboration on bringing XGrammar to vLLM and the continuous effort to improve structured decoding in vLLM.</p>

<h2 id="references">References</h2>

<ul>
  <li>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2016). <em>Neural Machine Translation by Jointly Learning to Align and Translate</em>. arXiv preprint arXiv:1409.0473</li>
  <li>Haugeland, J. (1997). <em>Mind Design II: Philosophy, Psychology, and Artificial Intelligence</em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/4626.001.0001">https://doi.org/10.7551/mitpress/4626.001.0001</a></li>
  <li>Hendler, J. (2008). Avoiding Another AI Winter. <em>IEEE Intelligent Systems</em>, <em>23</em>(2), 2–4. <a href="https://doi.org/10.1109/MIS.2008.20">https://doi.org/10.1109/MIS.2008.20</a></li>
  <li>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Computation</em>.</li>
  <li>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., &amp; Amodei, D. (2020). <em>Scaling Laws for Neural Language Models</em>. arXiv preprint arXiv:2001.08361</li>
  <li>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient Estimation of Word Representations in Vector Space</em>. arXiv preprint arXiv:1301.3781</li>
  <li>Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological Review</em>, <em>65</em>(6), 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a></li>
  <li>Rumelhart, D. E., McClelland, J. L., &amp; Group, P. R. (1986). <em>Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations</em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/5236.001.0001">https://doi.org/10.7551/mitpress/5236.001.0001</a></li>
  <li>Shortliffe, E. H. (1974). <em>MYCIN: A Rule-Based Computer Program for Advising Physicians Regarding Antimicrobial Therapy Selection</em> (Technical Report STAN-CS-74-465). Stanford University.</li>
  <li>Statistical Machine Translation. (n.d.). <em>IBM Models</em>. Statistical Machine Translation Survey. <a href="http://www2.statmt.org/survey/Topic/IBMModels">http://www2.statmt.org/survey/Topic/IBMModels</a></li>
  <li>Turing, A. M. (1950). i.—Computing Machinery And Intelligence. <em>Mind</em>, <em>LIX</em>(236), 433–460. <a href="https://doi.org/10.1093/mind/LIX.236.433">https://doi.org/10.1093/mind/LIX.236.433</a></li>
  <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2023). <em>Attention Is All You Need</em>. arXiv preprint arXiv:1706.03762</li>
  <li>Willard, B. T., &amp; Louf, R. (2023). <em>Efficient Guided Generation for Large Language Models</em>. arXiv preprint arXiv:2307.09702</li>
</ul>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p>Allen Newell and Herbert Simon’s work at RAND initially showed that computers can simulate important aspects of intelligence.</p>

      <p>Another notable application was found in the medical domain (Haugeland, 1997). MYCIN, developed at Stanford University in the 1970s, diagnosed and recommended treatments for blood infections (Shortliffe, 1974). MYCIN’s developers recognized the importance of justifying recommendations, implementing what were known as “rule traces” to explain the system’s reasoning in human-understandable terms. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p>In the 1990s, IBM released a sequence of complex statistical models that is trained to perform machine translations <a href="https://en.wikipedia.org/wiki/IBM_alignment_models">tasks</a> (Statistical Machine Translation, n.d.) (see also: this <a href="https://www.cs.cornell.edu/courses/cs5740/2017sp/lectures/08-alignments.pdf">lecture</a> from Cornell).</p>

      <p>In 2001, Bag of words (BoW)-variants model was trained on 0.3B tokens and was considered SOTA at the time (Mikolov et al., 2013). These earlier works proved to the research community that statistical modelling triumphs over symbolic counterpart for language processing given it can capture the general patterns for large corpuses of text. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p>In 2017, The landmark paper “Attention is all You Need” introduced Transformers architecture (Vaswani et al., 2023) for neural machine translations tasks, which is based on the attention mechanism first proposed by (Bahdanau et al., 2016).</p>

      <p>OpenAI then introduced the scaling law for neural language models (Kaplan et al., 2020), which sets off the race towards building these systems based on foundational language models. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">

      <p>Prior to Attention-based transformers, seq-to-seq models uses RNNs given its ability for longer context length and better memory. However, they are more susceptible to vanishing/exploding gradients comparing to feed-forward network, and thus LSTM (Hochreiter &amp; Schmidhuber, 1997) was proposed to solve this problem. Yet, one of the main problems with LSTM is that they tend to have poor memory recall with data they have seen many steps ago.</p>

      <p>The Attention paper addresses this problem by encoding additional positional data into the inputs. The paper also additionally proposed a encoder-decoder architecture for translation tasks, however, most of text-generation models nowadays are decoder-only, given its superior performance over zero-shot tasks.</p>

      <p>One of the many reasons why attention-based transformers works better than LSTM is because transformers are very scalable and hardware-aware (you can’t just arbitrary add more LSTM block and hope for better long-term retention). For more information, please refer back to the original paper. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">

      <p>One might argue that we can reliably achieve these through few-shot promptings, i.e “Give me a JSON that yields the address of users. Example output can be …”. However, there is no guarantee that the generated outputs is a valid JSON. This is because these models are probabilistic systems, as they are “sampling” the next results based on the distribution of data that it was trained on.</p>

      <p>One might also argue that one should use specific fine-tuned models for JSON outputs to perform such cases. However, fine-tuning often requires extensive training and a lot more labor to curate data, monitor progress, and perform evaluation, which is a huge resources not everyone can afford to do. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Note that the phrase “[structured/constrained/guided] decoding” are used interchangeably, but they all refer to the same mechanism of “using a format for the model to structurally sampling outputs.” <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>See this <a href="https://huggingface.co/blog/logits-processor-zoo">blog post</a> from HuggingFace for using logit processors to control the generation process. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/2025/01/14/struct-decode-intro.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="https://blog.vllm.ai/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p> -->
        <ul class="contact-list">
          <li class="p-name">© 2024. vLLM Team. All rights reserved.</li>
          <li><a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
